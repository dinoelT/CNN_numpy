{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "name": "ConvCNN_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbjX-IPpXeC9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install sparse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0g6Vltg7XYZz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "from Classes.ConvolutionLayer import Conv\n",
        "from Classes.MaxpoolLayer import Maxpool\n",
        "from Classes.FCLayer import FC\n",
        "from Classes.SoftmaxLayer import Softmax\n",
        "from Classes.ActivationLayer import Activation \n",
        "#import matplotlib.pyplot as plt\n",
        "from Classes.MiscFunc import *\n",
        "from Classes.DropoutLayer import Dropout\n",
        "from PIL import Image\n",
        "import os\n",
        "import datetime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CPb1fbSjNU0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir Accuracy\n",
        "!mkdir Checkpoints"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2eZbJGrgpum",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxRxcAF84WUu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Copy dataset from google drive\n",
        "%cp -av /gdrive/My\\ Drive/SortedResized /content/SortedResized"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3WY5YO69dRa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def shuffleTrainingData():\n",
        "  global usedPhotosDict\n",
        "  global listOfAvailableClasses\n",
        "  usedPhotosDict = {}\n",
        "  listOfAvailableClasses = list(range(len(folders)))\n",
        "  for folder in folders:\n",
        "      trainSize = len(os.listdir(rootFolder+'/'+folder+'/train'))\n",
        "      \n",
        "      statDict = {}\n",
        "      \n",
        "      trainOrd = np.random.permutation(np.arange(trainSize))\n",
        "\n",
        "      statDict[\"train\"] = [0, trainSize, trainOrd]\n",
        "      usedPhotosDict[folder] = statDict\n",
        "  #print(usedPhotosDict)\n",
        "  print(\"Images shuffled!\")\n",
        "\n",
        "def getRandomIndex(classInterval):\n",
        "    maxim = classInterval[-1]\n",
        "    \n",
        "    randomNr = np.random.randint(maxim)\n",
        "    #print(randomNr)\n",
        "    for i in range(1,len(classInterval)):\n",
        "        if(randomNr < classInterval[i] and randomNr > classInterval[i-1]):\n",
        "            return i-1\n",
        "\n",
        "\n",
        "def getNewBatch(batchType = \"train\"):\n",
        "    global usedPhotosDict\n",
        "    global trainClassIntervals\n",
        "    global testClassIntervals\n",
        "    global listOfAvailableClasses\n",
        "    \n",
        "    batch = np.zeros((batchSize, *imageSize))\n",
        "    if(len(listOfAvailableClasses) != 0):  \n",
        "        labels = [] \n",
        "        for i in range(batchSize):\n",
        "      \n",
        "            \n",
        "            foundImg = 0\n",
        "            while(foundImg == 0):\n",
        "                if(batchType == \"train\"):\n",
        "                    label = getRandomIndex(trainClassIntervals)\n",
        "                elif(batchType == \"test\"):\n",
        "                    pass\n",
        "                    #label = getRandomIndex(testClassIntervals)\n",
        "                else:\n",
        "                    print(\"No such batch type!\")\n",
        "                if(label in listOfAvailableClasses):\n",
        "                    foundImg = 1\n",
        "            \n",
        "            labels.append(label)\n",
        "            folder = folders[label]\n",
        "            indexes = usedPhotosDict[folder][batchType]\n",
        "            currIndex, maxIndex, order = indexes \n",
        "            #print(folder, currIndex,\"/\",maxIndex)\n",
        "            path = rootFolder + \"/\" + folder + \"/\" + batchType + \"/\" + str(order[currIndex]) + \".jpg\"\n",
        "            img = np.asarray(Image.open(path)) \n",
        "\n",
        "            batch[i] = np.transpose(img, axes = (2,1,0))\n",
        "\n",
        "            usedPhotosDict[folder][batchType][0] = currIndex + 1 \n",
        "\n",
        "            if(currIndex + 1 == maxIndex):\n",
        "                #This class has no more available photos\n",
        "                listOfAvailableClasses.remove(label)\n",
        "        if(len(labels) == batchSize):\n",
        "            labels = np.array(labels).astype(int)\n",
        "            return batch, labels\n",
        "        else:\n",
        "            print(\"Not enough images to complete batch\")\n",
        "            return 0\n",
        "    else:\n",
        "        print(\"No more available \"+batchType+\"ing examples\")\n",
        "        return 0\n",
        "\n",
        "def cnn_forward_test(img, label):\n",
        "    out = conv1.forward(img)\n",
        "    out = maxpool1.forward(out)\n",
        "    out = relu1.forward(out)\n",
        "\n",
        "    out = conv2.forward(out)\n",
        "    out = maxpool2.forward(out)\n",
        "    out = relu2.forward(out)\n",
        "    out = out.reshape(batchSize,-1)\n",
        "\n",
        "    out = fc1.forward(out)\n",
        "    out = fc2.forward(out)\n",
        "    out = softmax.forward(out)\n",
        "    \n",
        "    predicted = np.argmax(out,axis = 1)\n",
        "    #print(label, predicted)\n",
        "    acc = batchSize - np.count_nonzero(predicted - label)\n",
        "        \n",
        "    loss = crossEntropyLossForward(out, label)\n",
        "    \n",
        "    return acc, loss, out  \n",
        "\n",
        "def cnn_forward(img, label):\n",
        "    out = conv1.forward(img)\n",
        "    out = drop1.forward(out)\n",
        "    out = maxpool1.forward(out)\n",
        "    out = relu1.forward(out)\n",
        "\n",
        "    out = conv2.forward(out)\n",
        "    out = drop2.forward(out)\n",
        "    out = maxpool2.forward(out)\n",
        "    out = relu2.forward(out)\n",
        "    out = out.reshape(batchSize,-1)\n",
        "\n",
        "    out = fc1.forward(out)\n",
        "    out = drop3.forward(out)\n",
        "    out = fc2.forward(out)\n",
        "    out = softmax.forward(out)\n",
        "    \n",
        "    predicted = np.argmax(out,axis = 1)\n",
        "    #print(label, predicted)\n",
        "    acc = batchSize - np.count_nonzero(predicted - label)\n",
        "        \n",
        "    loss = crossEntropyLossForward(out, label)\n",
        "    \n",
        "    return acc, loss, out\n",
        "\n",
        " \n",
        "def cnn_backprop(dLdOut, lr):\n",
        "    out = softmax.backprop(dLdOut)    \n",
        "    out = fc2.backprop(out, lr)\n",
        "    out = drop3.backprop(out)\n",
        "    out = fc1.backprop(out, lr).reshape(batchSize * 100, 1, 16, 22)\n",
        "    out = relu2.backprop(out)\n",
        "    out = maxpool2.backprop(out)\n",
        "    out = drop2.backprop(out)\n",
        "    out = conv2.backprop(out, lr)\n",
        "    out = relu1.backprop(out)\n",
        "    out = maxpool1.backprop(out)\n",
        "    out = drop1.backprop(out)\n",
        "    out = conv1.backprop(out, lr)\n",
        "\n",
        "def saveNetwork(path):   \n",
        "    with h5py.File(path,'w') as f:\n",
        "        f.create_dataset('Conv1_W', data = conv1.W)\n",
        "        f.create_dataset('Conv2_W', data = conv2.W)\n",
        "        f.create_dataset('FC1_W', data = fc1.W)\n",
        "        f.create_dataset('FC1_B', data = fc1.B)\n",
        "        f.create_dataset('FC2_W', data = fc2.W)\n",
        "        f.create_dataset('FC2_B', data = fc2.B)\n",
        "\n",
        "def train(nameSave, errIndex, lr):\n",
        "  print(\"Start Training\")\n",
        "  #lr = 0.005\n",
        "\n",
        "  x = datetime.datetime.now()\n",
        "  data = x.strftime(\"%d %B %H:%M\") \n",
        "  print(data)\n",
        "\n",
        "  lossFile = open(\"Accuracy/loss\"+str(errIndex)+\".txt\",\"a\")\n",
        "  accFile = open(\"Accuracy/acc\"+str(errIndex)+\".txt\",\"a\")\n",
        "\n",
        "  accFile.write(\"BatchSize = 5\\n\"+data+\"\\n\") \n",
        "\n",
        "  shuffleTrainingData()\n",
        "\n",
        "  finished = 0\n",
        "\n",
        "  while(finished == 0):  \n",
        "      returned = getNewBatch()\n",
        "\n",
        "      if(returned != 0):\n",
        "          batch, lb = returned\n",
        "          accuracy, loss, out = cnn_forward(batch/255 - 0.5, lb)\n",
        "          \n",
        "          lossSum = np.sum(loss)\n",
        "\n",
        "          lossFile.write(str(lossSum)+\"\\n\")\n",
        "          accFile.write(str(accuracy)+\"\\n\")\n",
        "\n",
        "          lossFile.flush()\n",
        "          os.fsync(lossFile.fileno())\n",
        "\n",
        "          accFile.flush()\n",
        "          os.fsync(accFile.fileno())\n",
        "\n",
        "          print(lossSum, accuracy)\n",
        "          dLdOut = crossEntropyLossBackprop(out, lb)\n",
        "          cnn_backprop(dLdOut, lr)\n",
        "      else:\n",
        "          finished = 1\n",
        "\n",
        "  lossFile.close()\n",
        "  accFile.close()\n",
        "\n",
        "  saveNetwork(\"Checkpoints/\" + nameSave)\n",
        "  print(\"Finished Training\")\n",
        "\n",
        "testErrorDict = {}\n",
        "\n",
        "def resetTestErrorDict():\n",
        "  global testErrorDict\n",
        "  testErrorDict = {}\n",
        "\n",
        "  for folder in folders:\n",
        "    nrImages = len(os.listdir(rootFolder+'/'+folder+'/test'))\n",
        "    nrFullBatches = nrImages // batchSize\n",
        "\n",
        "    maxImage = nrImages - (nrImages % nrFullBatches)\n",
        "\n",
        "\n",
        "    testErrorDict[folder] = [0, maxImage]\n",
        "\n",
        "  print(testErrorDict)\n",
        "\n",
        "\n",
        "def getTestBatches():\n",
        "  for lb, value in enumerate(testErrorDict.values()):\n",
        "    maxImage = value[1]\n",
        "    for batchNr in range(0, maxImage, batchSize):\n",
        "      batch = np.zeros((batchSize, *imageSize))\n",
        "      labels = []\n",
        "      \n",
        "      for i in range(batchSize):\n",
        "        labels.append(lb)\n",
        "        folder = folders[lb]\n",
        "        path = rootFolder + \"/\" + folder + \"/test/\" + str(batchNr+i) + \".jpg\"\n",
        "        img = np.asarray(Image.open(path)) \n",
        "        batch[i] = np.transpose(img, axes = (2,1,0))\n",
        "      yield batch, labels\n",
        "\n",
        "\n",
        "#######################################\n",
        "#test\n",
        "\n",
        "def test(saveLogIndex):\n",
        "  global nrOfRun\n",
        "  lossFile1 = open(\"Accuracy/loss_test\"+str(saveLogIndex)+\".txt\",\"a\")\n",
        "  accFile1 = open(\"Accuracy/acc_test\"+str(saveLogIndex)+\".txt\",\"a\")\n",
        "  resetTestErrorDict()\n",
        "  accFile1.write(\"BatchSize = 5\\n\")\n",
        "\n",
        "  for batch, lb in getTestBatches():\n",
        "      accuracy, loss, out = cnn_forward_test(batch/255 - 0.5, lb)\n",
        "      \n",
        "      testErrorDict[folders[lb[0] ]][0] += accuracy\n",
        "      \n",
        "      lossSum = np.sum(loss)\n",
        "\n",
        "      print(lossSum, accuracy)\n",
        "\n",
        "      lossFile1.write(str(lossSum)+\"\\n\")\n",
        "      accFile1.write(str(accuracy)+\"\\n\")\n",
        "\n",
        "      lossFile1.flush() \n",
        "      os.fsync(lossFile1.fileno())\n",
        "\n",
        "      accFile1.flush()\n",
        "      os.fsync(accFile1.fileno())\n",
        "\n",
        "  for key in testErrorDict:\n",
        "    accFile1.write(str(key)+ str(testErrorDict[key]))\n",
        "\n",
        "  print(testErrorDict)\n",
        "\n",
        "  lossFile1.close()\n",
        "  accFile1.close()\n",
        "\n",
        "def loadWeights(file):\n",
        "    with h5py.File(file,'r') as f:\n",
        "    #ls = list(f.keys())   \n",
        "        conv1.W = np.array(f.get('Conv1_W')) \n",
        "        conv2.W = np.array(f.get('Conv2_W'))\n",
        "        fc1.W = np.array(f.get('FC1_W'))\n",
        "        fc1.B = np.array(f.get('FC1_B'))\n",
        "        fc2.W = np.array(f.get('FC2_W'))\n",
        "        fc2.B = np.array(f.get('FC2_B'))\n",
        "    \n",
        "    print(\"Weights loaded from\", file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxkNp-sYXYZ8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "84bb7285-ee92-484a-d23e-e4e3f8b7d093"
      },
      "source": [
        "rootFolder = \"SortedResized\"\n",
        "folders = os.listdir(rootFolder)\n",
        "print(folders)\n",
        "usedPhotosDict = {}\n",
        "listOfAvailableClasses = []\n",
        "\n",
        "shuffleTrainingData()\n",
        "\n",
        "#------------------------------------------------------\n",
        "trainClassIntervals = [0]\n",
        "\n",
        "for value in usedPhotosDict.values():\n",
        "    trainNr = value[\"train\"][1]\n",
        "    trainClassIntervals.append(trainClassIntervals[-1] + trainNr)\n",
        "    \n",
        "print(trainClassIntervals)\n",
        "\n",
        "batchSize = 5\n",
        "imageSize = (3, 144, 192)\n",
        "\n",
        "##### Init layers\n",
        "print(\"Initiate layers\")\n",
        "conv1 = Conv(10, (batchSize,3,144,192), batchSize=1, depth = 3,optimizer = 'momentum', filterSize = 6, stride = 2, isFirstLayer = 1)\n",
        "#output = (batchSize*10, 1, 70, 94)\n",
        "drop1 = Dropout(0.9)\n",
        "maxpool1 = Maxpool()\n",
        "#output = (batchSize*10, 1, 44, 59)\n",
        "relu1 = Activation(activation = 'leaky_relu')\n",
        "\n",
        "conv2 = Conv(10, (batchSize * 10, 1, 35, 47), batchSize=1, depth = 1,optimizer = 'momentum', filterSize = 4, stride = 1, isFirstLayer = 0)\n",
        "#output = (batchSize * 100, 1, 32, 44)\n",
        "drop2 = Dropout(0.9)\n",
        "maxpool2 = Maxpool()\n",
        "relu2 = Activation(activation = 'leaky_relu')\n",
        "#output = (batchSize * 100, 1, 16, 22)\n",
        "# flatten size = batchSize * 100 * 16 *22 = 176000 = batchSize * 35200\n",
        "fc1 = FC(35200, 100, batchSize=batchSize,optimizer = 'momentum')\n",
        "drop3 = Dropout(0.5)\n",
        "fc2 = FC(100, 5, batchSize=batchSize,optimizer = 'momentum')\n",
        "softmax = Softmax()\n",
        "print(\"Layer initialization done\")\n",
        "\n",
        "\n",
        "loadWeights(\"CNN_9_July9_acc82.h5\")\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Left', 'NoAttention', 'Front', 'Phone', 'Right']\n",
            "Images shuffled!\n",
            "[0, 275, 604, 914, 1321, 1565]\n",
            "Initiate layers\n",
            "Layer initialization done\n",
            "Weights loaded from CNN_9_July9_acc82.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1HxDhUcXYaB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "57a5e5a9-bd3a-40b7-aa00-359f40eb36c1"
      },
      "source": [
        "lr = 0.0015\n",
        "i=0\n",
        "train(\"CNN_10_lr_0015_.h5\", 18+i)\n",
        "test(9+i)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start Training\n",
            "10 July 17:17\n",
            "Images shuffled!\n",
            "0.011928435756969944 5\n",
            "0.0902162596215424 5\n",
            "0.006343438617545647 5\n",
            "0.005928468811095306 5\n",
            "0.028938391609465527 5\n",
            "0.9725718792597835 4\n",
            "0.09388481689585183 5\n",
            "0.007727703579710677 5\n",
            "0.01593870045159007 5\n",
            "0.04702581590496473 5\n",
            "0.00565247255590013 5\n",
            "4.070898829143249 3\n",
            "0.027784184116702998 5\n",
            "0.04649119017537147 5\n",
            "0.059805286186792705 5\n",
            "0.005737448331583178 5\n",
            "0.0017084132773942456 5\n",
            "0.005174324269026342 5\n",
            "0.07712913166793722 5\n",
            "0.1731219455209907 5\n",
            "0.23402414455256937 5\n",
            "0.17393450435358807 5\n",
            "0.02854338202355072 5\n",
            "0.0451643678787888 5\n",
            "0.0617143717403648 5\n",
            "0.0019007934549351453 5\n",
            "0.007861753030682235 5\n",
            "0.29606972244646446 5\n",
            "0.07140691610823884 5\n",
            "0.8587560518577344 4\n",
            "0.001295395304841438 5\n",
            "0.021711586628817288 5\n",
            "0.08624488445715671 5\n",
            "0.003621378881807525 5\n",
            "0.007337355047288355 5\n",
            "0.4067388595878573 5\n",
            "0.03773067912765009 5\n",
            "0.13182088860995167 5\n",
            "0.4817698664713369 5\n",
            "0.31626249426997816 5\n",
            "0.000574046003129879 5\n",
            "0.1680107388576629 5\n",
            "0.007529962881467474 5\n",
            "0.17939405178633136 5\n",
            "0.0017733222396519917 5\n",
            "0.332195535049724 5\n",
            "1.2368196773171596 4\n",
            "0.0002654529462900257 5\n",
            "0.003217685727669729 5\n",
            "0.03339909364160878 5\n",
            "0.0839611216835763 5\n",
            "0.08854092324229691 5\n",
            "0.040346781818058855 5\n",
            "0.005692688604691385 5\n",
            "0.04208262965681442 5\n",
            "0.033512361372599574 5\n",
            "0.12897314797033851 5\n",
            "5.299993828300556 4\n",
            "1.0983773086278128 4\n",
            "0.03087036858607028 5\n",
            "0.2925851315333976 5\n",
            "0.04684571518156391 5\n",
            "0.0023822256315371745 5\n",
            "0.006391708334709633 5\n",
            "0.13256372545998252 5\n",
            "0.3986865645967177 5\n",
            "3.922366757491727 4\n",
            "0.011900035406531765 5\n",
            "0.04783918080259912 5\n",
            "0.042865738953684555 5\n",
            "1.7418810680920211 4\n",
            "0.5449748076534886 5\n",
            "0.017246676523407795 5\n",
            "0.0982772307911511 5\n",
            "0.024570081558331056 5\n",
            "0.0005246256289199453 5\n",
            "0.8180745521981682 5\n",
            "0.01581924397590323 5\n",
            "0.04331062601692382 5\n",
            "0.0737215295482635 5\n",
            "0.03796252171883393 5\n",
            "0.3038604237760467 5\n",
            "0.010300195372393742 5\n",
            "0.0030491612266120406 5\n",
            "0.027503977276099642 5\n",
            "0.15874669335013056 5\n",
            "0.12069667661003984 5\n",
            "0.02729750653473181 5\n",
            "0.012660241997015055 5\n",
            "0.10941603300851567 5\n",
            "0.6252643321901246 5\n",
            "0.01990773936746564 5\n",
            "0.019409855922157524 5\n",
            "0.08744389926941548 5\n",
            "1.0431158866679155 4\n",
            "0.004542529550393833 5\n",
            "0.2251931314644617 5\n",
            "0.013410388041892833 5\n",
            "0.05795089959892915 5\n",
            "0.019993134643397793 5\n",
            "0.27857057215794745 5\n",
            "0.4010640627000413 5\n",
            "0.02686055934009639 5\n",
            "0.1874009525330412 5\n",
            "0.009925099611611813 5\n",
            "0.04652029375787593 5\n",
            "0.8617536892580275 5\n",
            "0.18130429409494747 5\n",
            "0.01735298304998943 5\n",
            "0.026319612180039812 5\n",
            "0.007394322652188557 5\n",
            "1.5553371599901833 4\n",
            "0.3487520731532818 5\n",
            "0.022848753480438636 5\n",
            "0.007943779337578886 5\n",
            "0.5119374888873607 5\n",
            "0.010669656364735037 5\n",
            "0.040412843116906405 5\n",
            "0.2935826252193897 5\n",
            "0.05596229813782032 5\n",
            "0.2634434892955638 5\n",
            "0.0025125725050198743 5\n",
            "0.026198671036000425 5\n",
            "0.00945485448598946 5\n",
            "0.0006827853006565368 5\n",
            "1.6001103392541054 4\n",
            "0.04825524250382768 5\n",
            "0.015591314537821137 5\n",
            "0.19915385970065316 5\n",
            "0.24811044794458273 5\n",
            "0.07136426252610831 5\n",
            "0.014079738041819006 5\n",
            "0.011805085588844645 5\n",
            "0.005000027459522258 5\n",
            "1.540059924893582 4\n",
            "0.034035731610813344 5\n",
            "0.3812992539593113 5\n",
            "0.0340658444555164 5\n",
            "0.10187179136016247 5\n",
            "0.10848893758851645 5\n",
            "0.05427499846730533 5\n",
            "0.11076726028999766 5\n",
            "0.6957372952553708 5\n",
            "0.16963294959266484 5\n",
            "0.6948550337115676 5\n",
            "0.010554411129220061 5\n",
            "0.26777967398547353 5\n",
            "0.40480937536596945 5\n",
            "0.06435759912181821 5\n",
            "0.39259822372985465 5\n",
            "0.005617393614690417 5\n",
            "0.19277986429073288 5\n",
            "0.7035072084975739 5\n",
            "0.00364164204306678 5\n",
            "0.010429252505944672 5\n",
            "0.024374388105654867 5\n",
            "0.12270444236397485 5\n",
            "0.09909903821293313 5\n",
            "0.016637759573634232 5\n",
            "0.4908935052290057 5\n",
            "0.19171337092987906 5\n",
            "0.004051053642904856 5\n",
            "0.4722597454122958 5\n",
            "0.014414222260272024 5\n",
            "0.38118287604460266 5\n",
            "0.025292733898481214 5\n",
            "0.0022349051931343437 5\n",
            "0.5328632356198686 5\n",
            "0.03271727175382451 5\n",
            "0.09517212716097838 5\n",
            "0.027388744671064905 5\n",
            "0.10532099361831987 5\n",
            "0.00803846127380306 5\n",
            "0.1419793877375321 5\n",
            "0.03473377933209117 5\n",
            "0.005196069196636278 5\n",
            "0.15673528870811773 5\n",
            "0.003444303126728914 5\n",
            "0.007520919740370389 5\n",
            "0.0357045502146462 5\n",
            "0.044184186374044254 5\n",
            "0.06559718246540094 5\n",
            "0.05272444585471986 5\n",
            "0.12584574077173932 5\n",
            "1.891394121167044 4\n",
            "0.03467133459116802 5\n",
            "1.5266477032961763 4\n",
            "0.061043905175158654 5\n",
            "0.008738101118777127 5\n",
            "0.01160714653457631 5\n",
            "0.005309952002568175 5\n",
            "0.0431072613895492 5\n",
            "0.1422649382024385 5\n",
            "0.010655168435711614 5\n",
            "0.04187578689237266 5\n",
            "0.20484397880149013 5\n",
            "0.021048864425543413 5\n",
            "0.010165276703286678 5\n",
            "0.057340472363521536 5\n",
            "0.20678495886832557 5\n",
            "0.003748915846461345 5\n",
            "0.05363183461583163 5\n",
            "0.13056958633674975 5\n",
            "0.37634267811720123 5\n",
            "0.08899111456761086 5\n",
            "0.5412982418410511 5\n",
            "0.014583973062694548 5\n",
            "0.004205018484253331 5\n",
            "0.1006177700424812 5\n",
            "0.05583055630438155 5\n",
            "0.13164281311325837 5\n",
            "0.001479865154378121 5\n",
            "1.1878740038218012 4\n",
            "0.013235028744148837 5\n",
            "2.244134323425049 4\n",
            "0.11810896674430775 5\n",
            "0.01625354511508006 5\n",
            "0.04429915444219046 5\n",
            "0.006570474323114303 5\n",
            "0.26565694977090815 5\n",
            "0.06234296641713456 5\n",
            "0.732269373186984 5\n",
            "0.012749859418040969 5\n",
            "0.5883775677115798 5\n",
            "0.19400038616629142 5\n",
            "0.10569324008055775 5\n",
            "0.542846061235612 5\n",
            "0.04952218081084234 5\n",
            "0.9258439989480354 5\n",
            "0.01301145077361333 5\n",
            "0.009429981867727933 5\n",
            "0.3155389491278328 5\n",
            "0.49623456743471256 5\n",
            "0.000762980858434731 5\n",
            "0.0033011289178991525 5\n",
            "0.02840739216882697 5\n",
            "0.003797036719044221 5\n",
            "0.043489055866031544 5\n",
            "0.053993969006091984 5\n",
            "0.0292351589634728 5\n",
            "0.002770003845469807 5\n",
            "0.26991291492939345 5\n",
            "0.0019374808720752773 5\n",
            "0.003058378720981004 5\n",
            "0.0026647561233608084 5\n",
            "0.002229717146914453 5\n",
            "0.7530131592547002 5\n",
            "0.06569921448473576 5\n",
            "0.010829964471440958 5\n",
            "0.02320264526234399 5\n",
            "0.5122747762874932 5\n",
            "0.013118498674138447 5\n",
            "0.06572917711352967 5\n",
            "0.0028688172801963788 5\n",
            "0.006987870082442059 5\n",
            "0.0024835115497341426 5\n",
            "1.5795708798191146 4\n",
            "0.028458112733120117 5\n",
            "0.011936021066562498 5\n",
            "0.00033394318846678055 5\n",
            "0.7033353695993763 4\n",
            "0.001988676189482362 5\n",
            "0.0778464191661767 5\n",
            "0.6731413240357584 5\n",
            "0.20323151775665124 5\n",
            "0.2597293580149606 5\n",
            "0.006578583197104167 5\n",
            "0.02110613739582265 5\n",
            "0.06805780169536978 5\n",
            "0.00562044433661945 5\n",
            "0.004462935579031999 5\n",
            "0.07780076245623443 5\n",
            "0.031507059120243944 5\n",
            "0.00013857383867075474 5\n",
            "0.029715682569610713 5\n",
            "0.03340807014269969 5\n",
            "0.8142437792636162 5\n",
            "0.006070064152276338 5\n",
            "0.04704185424002769 5\n",
            "1.5125602726603415 4\n",
            "0.091389233066148 5\n",
            "0.005659035581230614 5\n",
            "0.1292031548766416 5\n",
            "0.03899664896717503 5\n",
            "2.1540821132389314 4\n",
            "0.011093152007834412 5\n",
            "0.057118202140227683 5\n",
            "0.046157845054291775 5\n",
            "0.20864912990697596 5\n",
            "0.010436239289117482 5\n",
            "0.07253656883673983 5\n",
            "0.0008410889163976148 5\n",
            "0.01010194518391835 5\n",
            "0.014940073320250715 5\n",
            "0.011442596940160392 5\n",
            "0.007855872819421555 5\n",
            "0.2793672109786472 5\n",
            "0.03010243115297574 5\n",
            "0.758616579227937 5\n",
            "5.030517892163632 4\n",
            "0.0009158421395308615 5\n",
            "0.0012158544101106374 5\n",
            "0.06083644826957054 5\n",
            "0.04514323633327465 5\n",
            "0.04409992520419442 5\n",
            "0.006099043666681825 5\n",
            "0.0005137572525784251 5\n",
            "0.01499230650903622 5\n",
            "0.07541870464401955 5\n",
            "0.0022692168518112328 5\n",
            "0.0470854874241649 5\n",
            "0.07521131547606887 5\n",
            "0.037408555584172505 5\n",
            "No more available training examples\n",
            "Finished Training\n",
            "{'Left': [0, 115], 'NoAttention': [0, 140], 'Front': [0, 130], 'Phone': [0, 170], 'Right': [0, 100]}\n",
            "3.4931393844764034 4\n",
            "0.04629968136953946 5\n",
            "11.609213885354023 4\n",
            "7.519398580135446 4\n",
            "2.9067536714433593 4\n",
            "14.912557758217307 3\n",
            "0.06172064184246913 5\n",
            "0.05191837024373445 5\n",
            "0.29550194892761966 5\n",
            "0.5948807908132125 5\n",
            "0.3298243694733366 5\n",
            "0.13775011005044452 5\n",
            "8.597941368556343 3\n",
            "3.8144394075077024 4\n",
            "2.7608590659834458 3\n",
            "1.9464426665606596 4\n",
            "2.26946761376583 4\n",
            "4.9668273979568545 4\n",
            "0.060672631217183005 5\n",
            "2.794463173760806 4\n",
            "17.89935730530779 3\n",
            "1.6110140087486002 3\n",
            "2.993355504878307 4\n",
            "7.0777901509435805 3\n",
            "0.18825064217020837 5\n",
            "3.155762553336062 4\n",
            "5.957761134713805 4\n",
            "11.451147551744192 2\n",
            "0.242642571436254 5\n",
            "8.188993212611239 3\n",
            "3.2688261029384855 4\n",
            "0.7400070588482781 5\n",
            "0.05687322507654757 5\n",
            "10.745696537598064 3\n",
            "7.240024290925966 2\n",
            "1.656156425459296 4\n",
            "0.9123819391162172 4\n",
            "0.5686993260165507 5\n",
            "2.3564555548341475 4\n",
            "0.08346873714062394 5\n",
            "4.351173200973971 4\n",
            "12.543291869644762 3\n",
            "1.1835800870092856 4\n",
            "4.167896756716486 4\n",
            "3.732262110052768 4\n",
            "2.321734240667817 4\n",
            "2.9958480735237423 4\n",
            "8.524419189893079 3\n",
            "1.7271428513930753 4\n",
            "10.38114870909723 3\n",
            "8.348791204830283 3\n",
            "0.7746430282997987 4\n",
            "0.3656007701586549 5\n",
            "3.0929830184670393 4\n",
            "1.1793754235622453 5\n",
            "8.487357619430888 4\n",
            "7.569501676793065 2\n",
            "10.280180789145259 4\n",
            "0.0009472245847336135 5\n",
            "12.62834224965599 3\n",
            "2.150350469732157 4\n",
            "8.37204779734099 2\n",
            "0.29954397848617725 5\n",
            "1.5544248547417996 4\n",
            "2.219540303918565 4\n",
            "1.3983309704608953 4\n",
            "0.22311012579651418 5\n",
            "18.17492060790104 1\n",
            "1.7378667095339848 4\n",
            "0.032722252926424535 5\n",
            "7.799103248188629 4\n",
            "10.931550458274657 1\n",
            "1.562512104593201 4\n",
            "0.4996268897315106 5\n",
            "8.742516047720505 2\n",
            "0.13037012092438355 5\n",
            "6.1850084410802735 3\n",
            "13.144245345298545 2\n",
            "5.693459326922746 4\n",
            "6.142102089777527 2\n",
            "1.1715225660196174 4\n",
            "0.017239491764044983 5\n",
            "0.016757122270612775 5\n",
            "4.738993624127159 3\n",
            "0.3958623724120954 5\n",
            "4.3915537442356065 3\n",
            "0.5153033240410672 5\n",
            "10.649696127393378 3\n",
            "2.2448094871983124 4\n",
            "0.43870613757967236 5\n",
            "0.008047226861358297 5\n",
            "0.006266574510030214 5\n",
            "7.815404167852801 3\n",
            "2.2702976324274444 4\n",
            "1.8349291272850288 4\n",
            "0.18459988281639633 5\n",
            "0.015746986736341773 5\n",
            "0.4289947763283411 5\n",
            "5.960366362834152 4\n",
            "2.388576000595294 4\n",
            "0.11899993574492036 5\n",
            "0.07635001532892836 5\n",
            "4.008269590757445 4\n",
            "0.16553375591231043 5\n",
            "0.22338791562436322 5\n",
            "0.14182007292836565 5\n",
            "0.2195945476821866 5\n",
            "5.83346198427817 4\n",
            "3.6787376667154827 3\n",
            "0.08014779837707103 5\n",
            "1.5158977998678282 4\n",
            "0.06808587078949128 5\n",
            "2.1868742028690162 4\n",
            "4.205875353003662 4\n",
            "0.5151645645534275 5\n",
            "1.6730943390183635 4\n",
            "2.0963430498079076 4\n",
            "1.4805735614608309 4\n",
            "5.906527806436213 2\n",
            "0.7071363855289565 5\n",
            "0.4300359833261983 5\n",
            "0.1165403502296512 5\n",
            "1.8088061015385966 4\n",
            "0.6367066268287593 5\n",
            "2.3703648917629 4\n",
            "0.2537032592229836 5\n",
            "1.810327213605528 4\n",
            "0.25605542027551575 5\n",
            "1.527112059153771 4\n",
            "10.037293062513864 3\n",
            "2.6362671148698222 4\n",
            "{'Left': [95, 115], 'NoAttention': [107, 140], 'Front': [98, 130], 'Phone': [144, 170], 'Right': [85, 100]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIsr6uvHXYam",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#Check filter size and stride\n",
        "y,x = (35, 47)\n",
        "for filterSize in range(3,20):\n",
        "    for stride in range(1,6):\n",
        "        print(filterSize, stride, (y - filterSize) / stride + 1, (x - filterSize) / stride + 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNCo9uc1UvxT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "def "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}